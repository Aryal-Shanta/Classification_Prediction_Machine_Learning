{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#importing Pandas for Data manipulation\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Reading training data from csv file and storing in a variable\n",
    "df = pd.read_csv(\"train_set.csv\", encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>85389000</td>\n",
       "      <td>pdscpm gb part of panel of chiller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>85389000</td>\n",
       "      <td>nm  p economical extended rot hand parts for c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>85389000</td>\n",
       "      <td>lv ma pd trip unit for cvs parts of circuit br...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>85389000</td>\n",
       "      <td>lv na p trip unit for cvs switch parts of circ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>85389000</td>\n",
       "      <td>lv tmd pd trip unit for cvs parts of circuitbr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                               text\n",
       "0  85389000                 pdscpm gb part of panel of chiller\n",
       "1  85389000  nm  p economical extended rot hand parts for c...\n",
       "2  85389000  lv ma pd trip unit for cvs parts of circuit br...\n",
       "3  85389000  lv na p trip unit for cvs switch parts of circ...\n",
       "4  85389000  lv tmd pd trip unit for cvs parts of circuitbr..."
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#printing the 1st five rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#importing modules for data cleaning\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Defining regular expressions for removing unnecessay symbols, if any\n",
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
    "\n",
    "#Defining Stopwords to remove them from our data\n",
    "STOPWORDS = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Defining a function for cleaning the text data from training set\n",
    "def clean_text(text):\n",
    "    \n",
    "    text = BeautifulSoup(text, \"lxml\").text # HTML decoding\n",
    "    text = text.lower() # lowercase text\n",
    "    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace symbols defined above(if any) by space in text\n",
    "    text = BAD_SYMBOLS_RE.sub('', text) # remove symbols which are in BAD_SYMBOLS_RE defined above from text\n",
    "    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # remove stopwors from text\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Applying text cleaning on our training data\n",
    "df['text'] = df['text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Seperating Data into Training and Test Set\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.text\n",
    "y = df.label\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Preparing data for training\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "#Applying CountVectorizer to tokenize the text documents\n",
    "count_vect = CountVectorizer()\n",
    "X_train_count = count_vect.fit_transform(X_train)\n",
    "\n",
    "#Applying TfidfTransformer to learn the vocabulary and inverse document frequency weightings\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Using different Classifiers for training\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#Storing different models in a list\n",
    "models = [\n",
    "    ('NB',MultinomialNB()),\n",
    "    ('SVM',SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=0, max_iter=5, tol=None)),\n",
    "    ('LR',LogisticRegression(n_jobs=1, C=1e5)),\n",
    "    ('DT',DecisionTreeClassifier()),\n",
    "    ('kNN',KNeighborsClassifier()),\n",
    "    ('RF',RandomForestClassifier(n_estimators=200, max_depth=3, random_state=0)),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of different classifiers during Cross Validation\n",
      "NB: 0.885299\n",
      "SVM: 0.860375\n",
      "LR: 0.927707\n",
      "DT: 0.907683\n",
      "kNN: 0.882819\n",
      "RF: 0.483303\n"
     ]
    }
   ],
   "source": [
    "from sklearn import model_selection\n",
    "\n",
    "results = []\n",
    "names = []\n",
    "print(\"Accuracy of different classifiers during Cross Validation\")\n",
    "\n",
    "for name, model in models:\n",
    "    kfold = model_selection.KFold(n_splits=10, random_state=0)\n",
    "    cv_results = model_selection.cross_val_score(model, X_train_tfidf, y_train, cv=kfold, scoring='accuracy')\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg = \"%s: %f\" % (name, cv_results.mean())\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEVCAYAAAAb/KWvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJej/A4ASasBwJRh+AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x22d65b363c8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Compare Algorithms\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.suptitle('Algorithm Comparison')\n",
    "ax = fig.add_subplot(111)\n",
    "plt.boxplot(results)\n",
    "ax.set_xticklabels(names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 13850 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "#Using LSTM for training on Neural Network\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer, LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, LSTM, SpatialDropout1D, Embedding\n",
    "from keras.preprocessing import text, sequence\n",
    "\n",
    "from keras import utils\n",
    "\n",
    "# The maximum number of words to be used. (most frequent)\n",
    "MAX_NB_WORDS = 1000\n",
    "# Max number of words in each complaint.\n",
    "MAX_SEQUENCE_LENGTH = 32\n",
    "# This is fixed.\n",
    "EMBEDDING_DIM = 100\n",
    "\n",
    "tokenizer = text.Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n",
    "tokenizer.fit_on_texts(df['text'].values)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (23615, 32)\n"
     ]
    }
   ],
   "source": [
    "X = tokenizer.texts_to_sequences(df['text'].values)\n",
    "X = sequence.pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of data tensor:', X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of label tensor: (23615, 12)\n"
     ]
    }
   ],
   "source": [
    "Y = pd.get_dummies(df['label']).values\n",
    "print('Shape of label tensor:', Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16530, 32) (16530, 12)\n",
      "(7085, 32) (7085, 12)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_lstm, X_test_lstm, Y_train_lstm, Y_test_lstm = train_test_split(X,Y, test_size = 0.30, random_state = 42)\n",
    "print(X_train_lstm.shape,Y_train_lstm.shape)\n",
    "print(X_test_lstm.shape,Y_test_lstm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(12, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "batch_size = 32\n",
    "\n",
    "history = model.fit(X_train_lstm, Y_train_lstm, epochs=epochs, batch_size=batch_size,validation_split=0.1)\n",
    "\n",
    "accr = model.evaluate(X_test_lstm,Y_test_lstm)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))\n",
    "\n",
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"model_lstm.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "    \n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model_lstm.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#This training was done on Google Colab due to hardware constraints on local machine, and the output of training is pasted below\n",
    "\n",
    "Train on 14877 samples, validate on 1653 samples\n",
    "Epoch 1/100\n",
    "14877/14877 [==============================] - 46s 3ms/step - loss: 1.0577 - acc: 0.6499 - val_loss: 0.5453 - val_acc: 0.8294\n",
    "Epoch 2/100\n",
    "14877/14877 [==============================] - 41s 3ms/step - loss: 0.4451 - acc: 0.8542 - val_loss: 0.4052 - val_acc: 0.8609\n",
    "Epoch 3/100\n",
    "14877/14877 [==============================] - 40s 3ms/step - loss: 0.3538 - acc: 0.8814 - val_loss: 0.3440 - val_acc: 0.8887\n",
    "Epoch 4/100\n",
    "14877/14877 [==============================] - 41s 3ms/step - loss: 0.3056 - acc: 0.8994 - val_loss: 0.3414 - val_acc: 0.8869\n",
    "Epoch 5/100\n",
    "14877/14877 [==============================] - 40s 3ms/step - loss: 0.2798 - acc: 0.9055 - val_loss: 0.3141 - val_acc: 0.8972\n",
    "Epoch 6/100\n",
    "14877/14877 [==============================] - 40s 3ms/step - loss: 0.2558 - acc: 0.9139 - val_loss: 0.3188 - val_acc: 0.8935\n",
    "Epoch 7/100\n",
    "14877/14877 [==============================] - 41s 3ms/step - loss: 0.2331 - acc: 0.9184 - val_loss: 0.3239 - val_acc: 0.8941\n",
    "Epoch 8/100\n",
    "14877/14877 [==============================] - 40s 3ms/step - loss: 0.2218 - acc: 0.9244 - val_loss: 0.3306 - val_acc: 0.8966\n",
    "Epoch 9/100\n",
    "14877/14877 [==============================] - 40s 3ms/step - loss: 0.2114 - acc: 0.9251 - val_loss: 0.3202 - val_acc: 0.8966\n",
    "Epoch 10/100\n",
    "14877/14877 [==============================] - 40s 3ms/step - loss: 0.2010 - acc: 0.9313 - val_loss: 0.3265 - val_acc: 0.8984\n",
    "Epoch 11/100\n",
    "14877/14877 [==============================] - 40s 3ms/step - loss: 0.1915 - acc: 0.9316 - val_loss: 0.3153 - val_acc: 0.9044\n",
    "Epoch 12/100\n",
    "14877/14877 [==============================] - 40s 3ms/step - loss: 0.1812 - acc: 0.9371 - val_loss: 0.3339 - val_acc: 0.8941\n",
    "Epoch 13/100\n",
    "14877/14877 [==============================] - 40s 3ms/step - loss: 0.1746 - acc: 0.9374 - val_loss: 0.3185 - val_acc: 0.9014\n",
    "Epoch 14/100\n",
    "14877/14877 [==============================] - 40s 3ms/step - loss: 0.1652 - acc: 0.9422 - val_loss: 0.3299 - val_acc: 0.9002\n",
    "Epoch 15/100\n",
    "14877/14877 [==============================] - 40s 3ms/step - loss: 0.1602 - acc: 0.9425 - val_loss: 0.3398 - val_acc: 0.8996\n",
    "Epoch 16/100\n",
    "14877/14877 [==============================] - 40s 3ms/step - loss: 0.1546 - acc: 0.9461 - val_loss: 0.3280 - val_acc: 0.9056\n",
    "Epoch 17/100\n",
    "14877/14877 [==============================] - 40s 3ms/step - loss: 0.1542 - acc: 0.9455 - val_loss: 0.3374 - val_acc: 0.9026\n",
    "Epoch 18/100\n",
    "14877/14877 [==============================] - 40s 3ms/step - loss: 0.1478 - acc: 0.9484 - val_loss: 0.3473 - val_acc: 0.8959\n",
    "Epoch 19/100\n",
    "14877/14877 [==============================] - 40s 3ms/step - loss: 0.1380 - acc: 0.9507 - val_loss: 0.3480 - val_acc: 0.9038\n",
    "Epoch 20/100\n",
    "14877/14877 [==============================] - 40s 3ms/step - loss: 0.1359 - acc: 0.9511 - val_loss: 0.3409 - val_acc: 0.9044\n",
    "Epoch 21/100\n",
    "14877/14877 [==============================] - 40s 3ms/step - loss: 0.1329 - acc: 0.9515 - val_loss: 0.3502 - val_acc: 0.9032\n",
    "Epoch 22/100\n",
    "14877/14877 [==============================] - 40s 3ms/step - loss: 0.1276 - acc: 0.9562 - val_loss: 0.3570 - val_acc: 0.9068\n",
    "Epoch 23/100\n",
    "14877/14877 [==============================] - 40s 3ms/step - loss: 0.1273 - acc: 0.9554 - val_loss: 0.3648 - val_acc: 0.9020\n",
    "Epoch 24/100\n",
    "14877/14877 [==============================] - 40s 3ms/step - loss: 0.1221 - acc: 0.9562 - val_loss: 0.3799 - val_acc: 0.9032\n",
    "Epoch 25/100\n",
    "14877/14877 [==============================] - 40s 3ms/step - loss: 0.1178 - acc: 0.9586 - val_loss: 0.3658 - val_acc: 0.9032\n",
    "Epoch 26/100\n",
    "14877/14877 [==============================] - 40s 3ms/step - loss: 0.1174 - acc: 0.9571 - val_loss: 0.3751 - val_acc: 0.9050\n",
    "Epoch 27/100\n",
    "14877/14877 [==============================] - 40s 3ms/step - loss: 0.1110 - acc: 0.9605 - val_loss: 0.3843 - val_acc: 0.9056\n",
    "Epoch 28/100\n",
    "14877/14877 [==============================] - 39s 3ms/step - loss: 0.1107 - acc: 0.9596 - val_loss: 0.3761 - val_acc: 0.9002\n",
    "Epoch 29/100\n",
    "14877/14877 [==============================] - 40s 3ms/step - loss: 0.1061 - acc: 0.9613 - val_loss: 0.3857 - val_acc: 0.9056\n",
    "Epoch 30/100\n",
    "14877/14877 [==============================] - 40s 3ms/step - loss: 0.1061 - acc: 0.9620 - val_loss: 0.3753 - val_acc: 0.9062\n",
    "Epoch 31/100\n",
    "14877/14877 [==============================] - 40s 3ms/step - loss: 0.1055 - acc: 0.9616 - val_loss: 0.3922 - val_acc: 0.9080\n",
    "Epoch 32/100\n",
    "14877/14877 [==============================] - 40s 3ms/step - loss: 0.0995 - acc: 0.9638 - val_loss: 0.3862 - val_acc: 0.9068\n",
    "Epoch 33/100\n",
    "14877/14877 [==============================] - 40s 3ms/step - loss: 0.1020 - acc: 0.9626 - val_loss: 0.4079 - val_acc: 0.9056\n",
    "Epoch 34/100\n",
    "14877/14877 [==============================] - 40s 3ms/step - loss: 0.0985 - acc: 0.9640 - val_loss: 0.3953 - val_acc: 0.9105\n",
    "Epoch 35/100\n",
    "14877/14877 [==============================] - 40s 3ms/step - loss: 0.0985 - acc: 0.9646 - val_loss: 0.4144 - val_acc: 0.9014\n",
    "Epoch 36/100\n",
    "14877/14877 [==============================] - 39s 3ms/step - loss: 0.1005 - acc: 0.9634 - val_loss: 0.4137 - val_acc: 0.9056\n",
    "Epoch 37/100\n",
    "14877/14877 [==============================] - 39s 3ms/step - loss: 0.0932 - acc: 0.9665 - val_loss: 0.4287 - val_acc: 0.9038\n",
    "Epoch 38/100\n",
    "14877/14877 [==============================] - 40s 3ms/step - loss: 0.0927 - acc: 0.9665 - val_loss: 0.4279 - val_acc: 0.9062\n",
    "Epoch 39/100\n",
    "14877/14877 [==============================] - 39s 3ms/step - loss: 0.0934 - acc: 0.9658 - val_loss: 0.4357 - val_acc: 0.9014\n",
    "Epoch 40/100\n",
    "14877/14877 [==============================] - 40s 3ms/step - loss: 0.0920 - acc: 0.9666 - val_loss: 0.4326 - val_acc: 0.9068\n",
    "Epoch 41/100\n",
    "14877/14877 [==============================] - 40s 3ms/step - loss: 0.0925 - acc: 0.9661 - val_loss: 0.4322 - val_acc: 0.9020\n",
    "Epoch 42/100\n",
    "14877/14877 [==============================] - 40s 3ms/step - loss: 0.0878 - acc: 0.9675 - val_loss: 0.4339 - val_acc: 0.9050\n",
    "Epoch 43/100\n",
    "14877/14877 [==============================] - 39s 3ms/step - loss: 0.0884 - acc: 0.9661 - val_loss: 0.4370 - val_acc: 0.9074\n",
    "Epoch 44/100\n",
    "14877/14877 [==============================] - 39s 3ms/step - loss: 0.0861 - acc: 0.9684 - val_loss: 0.4413 - val_acc: 0.9099\n",
    "Epoch 45/100\n",
    "14877/14877 [==============================] - 40s 3ms/step - loss: 0.0854 - acc: 0.9690 - val_loss: 0.4379 - val_acc: 0.9044\n",
    "Epoch 46/100\n",
    "14877/14877 [==============================] - 39s 3ms/step - loss: 0.0874 - acc: 0.9677 - val_loss: 0.4432 - val_acc: 0.9044\n",
    "Epoch 47/100\n",
    "14877/14877 [==============================] - 39s 3ms/step - loss: 0.0822 - acc: 0.9697 - val_loss: 0.4477 - val_acc: 0.9056\n",
    "Epoch 48/100\n",
    "14877/14877 [==============================] - 39s 3ms/step - loss: 0.0845 - acc: 0.9683 - val_loss: 0.4493 - val_acc: 0.9038\n",
    "Epoch 49/100\n",
    "14877/14877 [==============================] - 39s 3ms/step - loss: 0.0827 - acc: 0.9693 - val_loss: 0.4503 - val_acc: 0.9032\n",
    "Epoch 50/100\n",
    "14877/14877 [==============================] - 39s 3ms/step - loss: 0.0835 - acc: 0.9679 - val_loss: 0.4455 - val_acc: 0.9032\n",
    "Epoch 51/100\n",
    "14877/14877 [==============================] - 40s 3ms/step - loss: 0.0819 - acc: 0.9694 - val_loss: 0.4483 - val_acc: 0.9056\n",
    "Epoch 52/100\n",
    "14877/14877 [==============================] - 39s 3ms/step - loss: 0.0810 - acc: 0.9698 - val_loss: 0.4631 - val_acc: 0.9020\n",
    "Epoch 53/100\n",
    "14877/14877 [==============================] - 39s 3ms/step - loss: 0.0815 - acc: 0.9694 - val_loss: 0.4647 - val_acc: 0.9062\n",
    "Epoch 54/100\n",
    "14877/14877 [==============================] - 40s 3ms/step - loss: 0.0803 - acc: 0.9702 - val_loss: 0.4696 - val_acc: 0.9080\n",
    "Epoch 55/100\n",
    "14877/14877 [==============================] - 40s 3ms/step - loss: 0.0833 - acc: 0.9689 - val_loss: 0.4874 - val_acc: 0.9038\n",
    "Epoch 56/100\n",
    "14877/14877 [==============================] - 39s 3ms/step - loss: 0.0803 - acc: 0.9687 - val_loss: 0.4710 - val_acc: 0.9056\n",
    "Epoch 57/100\n",
    "14877/14877 [==============================] - 39s 3ms/step - loss: 0.0782 - acc: 0.9701 - val_loss: 0.4798 - val_acc: 0.9056\n",
    "Epoch 58/100\n",
    "14877/14877 [==============================] - 39s 3ms/step - loss: 0.0781 - acc: 0.9702 - val_loss: 0.4925 - val_acc: 0.9044\n",
    "Epoch 59/100\n",
    "14877/14877 [==============================] - 40s 3ms/step - loss: 0.0779 - acc: 0.9698 - val_loss: 0.4696 - val_acc: 0.9087\n",
    "Epoch 60/100\n",
    "14877/14877 [==============================] - 39s 3ms/step - loss: 0.0767 - acc: 0.9707 - val_loss: 0.4729 - val_acc: 0.9044\n",
    "Epoch 61/100\n",
    "14877/14877 [==============================] - 40s 3ms/step - loss: 0.0779 - acc: 0.9707 - val_loss: 0.4786 - val_acc: 0.9080\n",
    "Epoch 62/100\n",
    "14877/14877 [==============================] - 39s 3ms/step - loss: 0.0787 - acc: 0.9698 - val_loss: 0.4770 - val_acc: 0.9074\n",
    "Epoch 63/100\n",
    "14877/14877 [==============================] - 39s 3ms/step - loss: 0.0774 - acc: 0.9704 - val_loss: 0.4906 - val_acc: 0.9080\n",
    "Epoch 64/100\n",
    "14877/14877 [==============================] - 39s 3ms/step - loss: 0.0766 - acc: 0.9710 - val_loss: 0.4984 - val_acc: 0.9038\n",
    "Epoch 65/100\n",
    "14877/14877 [==============================] - 39s 3ms/step - loss: 0.0768 - acc: 0.9703 - val_loss: 0.4948 - val_acc: 0.9050\n",
    "Epoch 66/100\n",
    "14877/14877 [==============================] - 39s 3ms/step - loss: 0.0765 - acc: 0.9699 - val_loss: 0.4875 - val_acc: 0.9050\n",
    "Epoch 67/100\n",
    "14877/14877 [==============================] - 39s 3ms/step - loss: 0.0770 - acc: 0.9710 - val_loss: 0.4803 - val_acc: 0.9099\n",
    "Epoch 68/100\n",
    "14877/14877 [==============================] - 39s 3ms/step - loss: 0.0748 - acc: 0.9717 - val_loss: 0.4931 - val_acc: 0.9062\n",
    "Epoch 69/100\n",
    "14877/14877 [==============================] - 40s 3ms/step - loss: 0.0747 - acc: 0.9716 - val_loss: 0.4989 - val_acc: 0.9038\n",
    "Epoch 70/100\n",
    "14877/14877 [==============================] - 39s 3ms/step - loss: 0.0730 - acc: 0.9712 - val_loss: 0.4947 - val_acc: 0.9062\n",
    "Epoch 71/100\n",
    "14877/14877 [==============================] - 39s 3ms/step - loss: 0.0788 - acc: 0.9692 - val_loss: 0.4983 - val_acc: 0.9026\n",
    "Epoch 72/100\n",
    "14877/14877 [==============================] - 39s 3ms/step - loss: 0.0754 - acc: 0.9709 - val_loss: 0.4920 - val_acc: 0.9056\n",
    "Epoch 73/100\n",
    "14877/14877 [==============================] - 39s 3ms/step - loss: 0.0734 - acc: 0.9719 - val_loss: 0.4985 - val_acc: 0.9056\n",
    "Epoch 74/100\n",
    "14877/14877 [==============================] - 39s 3ms/step - loss: 0.0746 - acc: 0.9710 - val_loss: 0.4901 - val_acc: 0.9062\n",
    "Epoch 75/100\n",
    "14877/14877 [==============================] - 39s 3ms/step - loss: 0.0767 - acc: 0.9714 - val_loss: 0.4853 - val_acc: 0.9032\n",
    "Epoch 76/100\n",
    "14877/14877 [==============================] - 40s 3ms/step - loss: 0.0733 - acc: 0.9708 - val_loss: 0.4976 - val_acc: 0.9062\n",
    "Epoch 77/100\n",
    "14877/14877 [==============================] - 39s 3ms/step - loss: 0.0731 - acc: 0.9708 - val_loss: 0.4876 - val_acc: 0.9074\n",
    "Epoch 78/100\n",
    "14877/14877 [==============================] - 40s 3ms/step - loss: 0.0723 - acc: 0.9720 - val_loss: 0.4921 - val_acc: 0.9044\n",
    "Epoch 79/100\n",
    "14877/14877 [==============================] - 39s 3ms/step - loss: 0.0745 - acc: 0.9704 - val_loss: 0.5017 - val_acc: 0.9020\n",
    "Epoch 80/100\n",
    "14877/14877 [==============================] - 40s 3ms/step - loss: 0.0755 - acc: 0.9708 - val_loss: 0.4814 - val_acc: 0.9044\n",
    "Epoch 81/100\n",
    "14877/14877 [==============================] - 39s 3ms/step - loss: 0.0726 - acc: 0.9716 - val_loss: 0.4815 - val_acc: 0.9008\n",
    "Epoch 82/100\n",
    "14877/14877 [==============================] - 39s 3ms/step - loss: 0.0740 - acc: 0.9716 - val_loss: 0.4903 - val_acc: 0.9056\n",
    "Epoch 83/100\n",
    "14877/14877 [==============================] - 40s 3ms/step - loss: 0.0735 - acc: 0.9722 - val_loss: 0.4873 - val_acc: 0.9062\n",
    "Epoch 84/100\n",
    "14877/14877 [==============================] - 40s 3ms/step - loss: 0.0718 - acc: 0.9724 - val_loss: 0.5009 - val_acc: 0.9038\n",
    "Epoch 85/100\n",
    "14877/14877 [==============================] - 39s 3ms/step - loss: 0.0737 - acc: 0.9708 - val_loss: 0.5009 - val_acc: 0.9026\n",
    "Epoch 86/100\n",
    "14877/14877 [==============================] - 40s 3ms/step - loss: 0.0714 - acc: 0.9724 - val_loss: 0.4976 - val_acc: 0.9056\n",
    "Epoch 87/100\n",
    "14877/14877 [==============================] - 40s 3ms/step - loss: 0.0735 - acc: 0.9716 - val_loss: 0.5011 - val_acc: 0.9038\n",
    "Epoch 88/100\n",
    "14877/14877 [==============================] - 40s 3ms/step - loss: 0.0731 - acc: 0.9712 - val_loss: 0.5013 - val_acc: 0.9044\n",
    "Epoch 89/100\n",
    "14877/14877 [==============================] - 40s 3ms/step - loss: 0.0737 - acc: 0.9722 - val_loss: 0.5069 - val_acc: 0.9014\n",
    "Epoch 90/100\n",
    "14877/14877 [==============================] - 41s 3ms/step - loss: 0.0714 - acc: 0.9719 - val_loss: 0.4964 - val_acc: 0.9087\n",
    "Epoch 91/100\n",
    "14877/14877 [==============================] - 40s 3ms/step - loss: 0.0713 - acc: 0.9726 - val_loss: 0.5027 - val_acc: 0.9038\n",
    "Epoch 92/100\n",
    "14877/14877 [==============================] - 40s 3ms/step - loss: 0.0732 - acc: 0.9709 - val_loss: 0.5114 - val_acc: 0.9032\n",
    "Epoch 93/100\n",
    "14877/14877 [==============================] - 40s 3ms/step - loss: 0.0735 - acc: 0.9713 - val_loss: 0.5033 - val_acc: 0.9056\n",
    "Epoch 94/100\n",
    "14877/14877 [==============================] - 40s 3ms/step - loss: 0.0715 - acc: 0.9730 - val_loss: 0.5108 - val_acc: 0.9026\n",
    "Epoch 95/100\n",
    "14877/14877 [==============================] - 40s 3ms/step - loss: 0.0709 - acc: 0.9725 - val_loss: 0.5126 - val_acc: 0.9074\n",
    "Epoch 96/100\n",
    "14877/14877 [==============================] - 40s 3ms/step - loss: 0.0733 - acc: 0.9714 - val_loss: 0.4911 - val_acc: 0.9074\n",
    "Epoch 97/100\n",
    "14877/14877 [==============================] - 40s 3ms/step - loss: 0.0718 - acc: 0.9720 - val_loss: 0.4953 - val_acc: 0.9056\n",
    "Epoch 98/100\n",
    "14877/14877 [==============================] - 40s 3ms/step - loss: 0.0707 - acc: 0.9722 - val_loss: 0.5042 - val_acc: 0.9087\n",
    "Epoch 99/100\n",
    "14877/14877 [==============================] - 40s 3ms/step - loss: 0.0708 - acc: 0.9710 - val_loss: 0.5168 - val_acc: 0.9038\n",
    "Epoch 100/100\n",
    "14877/14877 [==============================] - 39s 3ms/step - loss: 0.0715 - acc: 0.9717 - val_loss: 0.5122 - val_acc: 0.9044\n",
    "7085/7085 [==============================] - 6s 874us/step\n",
    "Test set\n",
    "  Loss: 0.514\n",
    "  Accuracy: 0.907\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the Cross Validation score for Logistic Regression Classifier is highest among others(~0.93) including LSTM which has a validation accuracy of 0.907, we use the Logistic Regression Classifier for making predictions on our test set.\n",
    "\n",
    "First checking the accuracy on the test set splitted form the existing training set in the ratio of Train(70%) and Test(30%). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.9338038108680311\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pickle\n",
    "\n",
    "logreg = Pipeline([('vect', CountVectorizer()),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "                ('clf', LogisticRegression(n_jobs=1, C=1e5)),\n",
    "               ])\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "#Saving the trained model to pickle file \n",
    "filename = \"logreg_model.pkl\"\n",
    "with open(filename, 'wb') as file:\n",
    "    pickle.dump(logreg, file)\n",
    "\n",
    "y_pred = logreg.predict(X_test)\n",
    "acc_lr = accuracy_score(y_pred, y_test)\n",
    "\n",
    "print('accuracy %s' % acc_lr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy obtained on test set(0.93) is near to the cross validation score of the Logistic Regression classifier obtained above.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using test_set.csv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we test the performance of the Logistic Regression model which gave the highest accuracy among other clasifier on a different test set. i.e., test_set.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>lv tmd pd trip unit for nh parts of circuit br...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>module tm analog outputs analog output expansi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>command group t iii mechanismt p parts forcir...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>parts of relayelectrical contact  issu e f xxup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>parts for programmable logic controllers  dm  ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  lv tmd pd trip unit for nh parts of circuit br...\n",
       "1  module tm analog outputs analog output expansi...\n",
       "2   command group t iii mechanismt p parts forcir...\n",
       "3    parts of relayelectrical contact  issu e f xxup\n",
       "4  parts for programmable logic controllers  dm  ..."
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#Reading testing data from csv file and storing in a variable\n",
    "df_test = pd.read_csv(\"test_set.csv\", encoding='latin-1')\n",
    "\n",
    "#printing 1st five rows\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#X_test_test stands for test data(text) from test_set\n",
    "X_test_test = df_test.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Making prediction on the test set using the Logistic Regression model trained above\n",
    "filename = \"logreg_model.pkl\"\n",
    "with open(filename, 'rb') as file:\n",
    "    logreg_model = pickle.load(file)\n",
    "\n",
    "y_pred = logreg_model.predict(X_test_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[85389000 85389000 85389000 ... 84713010 84713010 84713010]\n"
     ]
    }
   ],
   "source": [
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85369090,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85369090,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 39269099,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85369090,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85369090,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 87089900,\n",
       " 85389000,\n",
       " 39269099,\n",
       " 85389000,\n",
       " 39269099,\n",
       " 85389000,\n",
       " 87089900,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85369090,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85369090,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85366990,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 39269099,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85369090,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85369090,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85369090,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85364900,\n",
       " 85389000,\n",
       " 85369090,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85366990,\n",
       " 85389000,\n",
       " 73181500,\n",
       " 85366990,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 39269099,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85364900,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 39269099,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 39269099,\n",
       " 87082900,\n",
       " 87082900,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85366990,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85177090,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85364900,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85364900,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 87089900,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85369090,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85366990,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85369090,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85369090,\n",
       " 85389000,\n",
       " 85364900,\n",
       " 85389000,\n",
       " 87089900,\n",
       " 87089900,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 39269099,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85366990,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85369090,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 39269099,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 39269099,\n",
       " 39269099,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85366990,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 39269099,\n",
       " 85389000,\n",
       " 85369090,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 87089900,\n",
       " 87089900,\n",
       " 73181500,\n",
       " 73181500,\n",
       " 85369090,\n",
       " 85364900,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85369090,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85366990,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85364900,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85364900,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 39269099,\n",
       " 85369090,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 73181500,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85366990,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 87082900,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 73181500,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85366990,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85369090,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85369090,\n",
       " 85369090,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85369090,\n",
       " 85369090,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85389000,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85389000,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 39269099,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85389000,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 87082900,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85366990,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85369090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85389000,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " 85177090,\n",
       " ...]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the final predictions on the test set. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "nteract": {
   "version": "0.27.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
